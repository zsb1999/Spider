Python 爬虫基础
网络爬虫盗亦有道，爬虫有风险，编写须谨慎
第一章
1.resquests库( 自动爬取HTML页面，自动网络请求提交)
    1.1 requests库的get方法
        r = requests.get(url)  构造一个向服务器请求资源的Request对象，服务返回response对象
        request.get(url,params = None, **kwargs)
        url: 拟获取页面的url链接
        params: url中额外的参数，字典或字节流格式，可选
        **kwargs: 12个控制访问的参数
    1.2 Requests库最重要的两个对象 response对象和request对象
        response对象的属性
            r.status_code : HTTP请求的返回状态，200表示成功，else 表示失败
            r.text : HTTP响应内容的字符串格式，url对应的页面内容
            r.encoding : 猜测的编码方式
            r.apparent_encoding :从页面中分析出的响应内容编码方式
            r.content : 响应内容的二进制形式
2.爬虫通用代码框架
    网络连接有风险，异常处理很重要
    Requests库的6种异常
        1.requests.ConnectionError:网络连接异常，如DNS查询失败、拒绝连接等等
        2.requests.HTTPError:HTTP错误异常
        3.requests.URLRequired:URL缺失异常
        4.requests.TooManyRedirects:超过最大重定向次数，产生重定向异常
        5.requests.ConnectTimeout:连接远程服务器超时异常
        6.requests.Timeout:请求URL超时，产生超时异常
    通用代码框架：
        import requests
        def getHTTPText(url):
            try:
                r = requests.get(url)
                r.raise_for_status()
                r.encoding = r.apparent_encoding # 如果状态不是200，将产生HTTPError异常
                return r.text
            except:
                return "产生异常" 
        if __name__ == "__main__":
            url = "http://www.baidu.com"
            print(getHTTPText(url))
3.Requests库主要方法解析 实际上Requests库的7种方法对应于HTTP协议的7种请求方式
    1.requests.request(method, url,**kwargs) : 构造一个请求，支撑一下各种方法
        method: 请求方式，对应get/post/put等7种方法
            例如： r = requests.request('GET',url,**kwargs)
        url: 拟获取页面的url链接
        **kwargs: 控制访问的参数，13个
        params: 字典或字节流，作为参数添加到url中
        headers: 字典，HTTP定制请求头
        cookies：字典或CookieJar，Request中的cookie
        auth: 元组，支持HTTP认证功能
        files：字典类型，向服务器传输文件
        timeout：设定超时时间，秒为单位
        proxies：字典，设定访问代理服务器，可以增加登录认证
        data：
        json：
        还有几个开关参数，不常用，不再赘述
    2.requests.get(URL,params = None, **kwargs) : 
        url: 拟获取页面的url链接
        params: 字典或字节流，作为参数添加到url中
        **kwargs: 控制访问的参数，12个
    3.requests.post(url, data = None, json = None, **kwargs) :
        url: 拟更新页面的url链接
        data: 字典/字节序列/文件，Request的内容
        json：Json格式的数据，Request的内容
        **kwargs: 控制访问的参数，11个
    4.requests.head(url,**kwargs) :
        url: 拟获取页面的url链接
        **kwargs: 控制访问的参数，13个
    5.requests.put(utl,data =None, **kwargs) :
        url: 拟更新页面的url链接
        data: 字典/字节序列/文件，Request的内容
        **kwargs: 控制访问的参数，12个
    6.requests.patch(url, data = None, **kwargs) : 局部修改请求
        url: 拟更新页面的url链接
        data: 字典/字节序列/文件，Request的内容
        **kwargs: 控制访问的参数，12个
    7.requests.delete(url, **kwargs) :  删除请求
        url: 拟删除页面的url链接
        **kwargs: 控制访问的参数，13个
4.put()方法和patch()方法的区别
    如果有一个userinfo里面包含name/age/sex等等很多参数，现在你想修改其中一个参数
    put()方法需要将所有参数重新上传，即上传所有参数覆盖原来的参数
    patch()方法只需要上传您所需要修改的参数内容
小结：
    最常用的方法 get()/head()/post
    牢记爬虫通用代码框架

5.Robots协议
例如：京东的Robots协议 http://www.jd.com/robots.txt
爬虫有风险，Robots协议只是为了约束爬虫行为
如果你的爬取量很小，不对服务器造成骚扰，或者十分钟只访问服务器一次的类人类行为 可以不用估计Robots协议
最重要的一点是你爬取的资源不能用作商业，谋取利润等等
网络爬虫盗亦有道乎！

五个实例用作练习Requests库的用法
